{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sit-xinli/ai-course7/blob/main/GenAI_dpo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnSzY2OlRI79"
      },
      "source": [
        "# 生成AIモデルアライメント\n",
        "## 目的\n",
        "- ラベル付けされたプリファレンスデータを使用してモデルの動作を調整する方法を学ぶ。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcXimb2YRI8A"
      },
      "source": [
        "## 必要なライブラリのインストールとインポート (~2分)\n",
        "## ブロックが正常に終了した場合は警告を無視する。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZM1jhsyB_kR"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KKggQtnTRI8B",
        "outputId": "04831884-67e7-425f-cfc9-80e8fbf3a969",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n",
            "Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl (67.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n"
          ]
        }
      ],
      "source": [
        "!pip install -U bitsandbytes\n",
        "!pip install datasets peft trl accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Nxtm9AqRI8C"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import json\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "from peft import LoraConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig, GenerationConfig\n",
        "from tqdm.auto import tqdm\n",
        "from trl import DPOConfig, DPOTrainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhR0gJZBRI8D"
      },
      "source": [
        "## データセットを読み込む"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jHgIOYc9sA5"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/sit-xinli/ai-course7.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1OmziTQ9RI8D"
      },
      "outputs": [],
      "source": [
        "# Open and load the json dataset\n",
        "with open(\"/content/GenAI_hw6_dataset/labelled_data.json\", 'r') as jsonfile:\n",
        "    full_data = json.load(jsonfile)\n",
        "\n",
        "with open(\"/content/GenAI_hw6_dataset/test_prompt.json\", 'r') as jsonfile:\n",
        "    test_data = json.load(jsonfile)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJgIZhNiRI8E"
      },
      "source": [
        "## ロードモデル"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2VqCkEXRI8E"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "from google.colab import userdata\n",
        "API_TOKEN=userdata.get('HF_TOKEN')\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
        "    device_map='auto',\n",
        "    trust_remote_code=True,\n",
        "    quantization_config=BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type='nf4'\n",
        "    ),\n",
        "    use_auth_token=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6fTJw9BRI8F"
      },
      "source": [
        "## オリジナルモデルからのレスポンスを得る"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "P1nkgCcCRI8F"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
        "    use_auth_token=False\n",
        "    )\n",
        "tokenizer.padding_side = \"right\"\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def data_formulate(data):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": '回覆請少於20字'},\n",
        "        {\"role\": \"user\", \"content\": data['prompt']},\n",
        "    ]\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    return prompt\n",
        "\n",
        "original_model_response = []\n",
        "for data in tqdm(test_data):\n",
        "    id = data['id']\n",
        "    print(f'Question {id}:\\n'+data['prompt'])\n",
        "    inputs = tokenizer(data_formulate(data), return_tensors=\"pt\").to('cuda')\n",
        "    generation_config=GenerationConfig(\n",
        "            do_sample=False,\n",
        "            max_new_tokens = 200,\n",
        "            pad_token_id = tokenizer.pad_token_id\n",
        "    )\n",
        "    output = model.generate(**inputs, generation_config=generation_config)\n",
        "    output = tokenizer.batch_decode(output, skip_special_tokens=True)[0].split('assistant\\n')[1]\n",
        "\n",
        "    original_model_response.append(output)\n",
        "    print('Response from original model:\\n'+output+'\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6bav5VuRI8C"
      },
      "source": [
        "## パラメーターを設定する\n",
        "### このブロックだけを変更すればよい。 他の部分は変更しないでください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rbc-x1aKRI8D"
      },
      "outputs": [],
      "source": [
        "num_epoch = 1\n",
        "data_size = 50\n",
        "support_ratio = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSA2fmpEK1nW"
      },
      "source": [
        "## トレーニングデータの準備"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cjz12h8KsnV"
      },
      "outputs": [],
      "source": [
        "# Select part of the data for training\n",
        "training_data = full_data[:data_size]\n",
        "\n",
        "# Define the size of the support dataset\n",
        "support_data_size = int(data_size * support_ratio)\n",
        "\n",
        "# Prepare the data for the training dataset\n",
        "prompt_list = [data_formulate(data) for data in training_data]\n",
        "chosen_list = [data['support'] for data in training_data[:support_data_size]] + [data['oppose'] for data in training_data[support_data_size:]]\n",
        "rejected_list = [data['oppose'] for data in training_data[:support_data_size]] + [data['support'] for data in training_data[support_data_size:]]\n",
        "position_list = ['support' for _ in range(support_data_size)] + ['oppose' for _ in range(data_size - support_data_size)]\n",
        "\n",
        "# Create the training dataset\n",
        "train_dataset = Dataset.from_dict({'prompt': prompt_list, 'position': position_list, 'chosen': chosen_list, 'rejected': rejected_list})\n",
        "pd.DataFrame(train_dataset).rename(columns={\"chosen\": \"preferred\", \"rejected\": \"non-preferred\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIBhiTYQRI8F"
      },
      "source": [
        "## トレーニング"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPd6nrnBRI8F"
      },
      "outputs": [],
      "source": [
        "\n",
        "# DPOConfig を使ってトレーニング設定を定義\n",
        "training_args = DPOConfig(\n",
        "   output_dir='./',\n",
        "   per_device_train_batch_size=1,\n",
        "   num_train_epochs=num_epoch,\n",
        "   gradient_accumulation_steps=8,\n",
        "   gradient_checkpointing=False,\n",
        "   learning_rate=2e-4,\n",
        "   optim=\"paged_adamw_8bit\",\n",
        "   logging_steps=1,\n",
        "   warmup_ratio=0.1,\n",
        "   report_to=None,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdsDp5rkRI8G"
      },
      "outputs": [],
      "source": [
        "peft_config = LoraConfig(\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    r=64,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qi5le-IQRI8G"
      },
      "outputs": [],
      "source": [
        "# 推奨：このセルでコードを変更しないこと\n",
        "os.environ[\"WANDB_MODE\"] = \"offline\"\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# DPOTrainer の初期化（ref_model も必要です）\n",
        "dpo_trainer = DPOTrainer(\n",
        "    model=model,\n",
        "    #ref_model=ref_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    #eval_dataset=eval_dataset,# 任意\n",
        "    peft_config=peft_config,\n",
        "    processing_class=tokenizer# 任意（ログや保存に便利）\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzUnqb40RI8H"
      },
      "outputs": [],
      "source": [
        "dpo_trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I22TanExRI8H"
      },
      "source": [
        "## 学習済みモデルからの応答を得る"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PuPGVAquRI8H"
      },
      "outputs": [],
      "source": [
        "trained_model_response = []\n",
        "for data in tqdm(test_data):\n",
        "    id = data['id']\n",
        "    print(f'Question {id}:\\n'+data['prompt'])\n",
        "    inputs = tokenizer(data_formulate(data), return_tensors=\"pt\").to('cuda')\n",
        "    generation_config=GenerationConfig(\n",
        "            do_sample=False,\n",
        "            max_new_tokens = 200,\n",
        "            pad_token_id = tokenizer.pad_token_id\n",
        "    )\n",
        "    output = model.generate(**inputs, generation_config=generation_config)\n",
        "    output = tokenizer.batch_decode(output, skip_special_tokens=True)[0].split('assistant\\n')[1]\n",
        "    trained_model_response.append(output)\n",
        "    print('Response from trained model:\\n'+output+'\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qe-PtYZMRI8I"
      },
      "source": [
        "## レポートを完成させるために、このブロックの出力を観察し、結果のスクリーンショットを撮る。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1i0wI6rLRI8I"
      },
      "outputs": [],
      "source": [
        "model_response = []\n",
        "print(f'num_epoch: {num_epoch}\\ndata_size: {data_size}\\nsupport_ratio: {support_ratio}')\n",
        "print()\n",
        "for data in test_data:\n",
        "    id = data['id']\n",
        "    ref_output = original_model_response[id-1]\n",
        "    output = trained_model_response[id-1]\n",
        "    print(f'Question {id}:\\n'+data['prompt'])\n",
        "    print('Response from original model:\\n'+ref_output)\n",
        "    print('Response from trained model:\\n'+output)\n",
        "    print()\n",
        "    model_response.append({'id':data['id'], 'prompt':data['prompt'], 'response_from_original_model':ref_output, 'response_from_trained_model':output})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0FHblUvRI8I"
      },
      "source": [
        "## 出力ファイルの取得"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owGIuqdnRI8I"
      },
      "outputs": [],
      "source": [
        "with open(f\"epoch-{num_epoch}_size-{data_size}_ratio-{support_ratio}.json\", \"w\", encoding='UTF-8') as outfile:\n",
        "    json.dump(model_response, outfile, indent=4, ensure_ascii=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 4412564,
          "sourceId": 7580251,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30636,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}